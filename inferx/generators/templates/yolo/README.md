# YOLO ONNX Inference Project

This is an auto-generated YOLO inference project using InferX.

## ğŸš€ Quick Start

### Without API Server

1. **Install dependencies:**
   ```bash
   uv sync
   ```

2. **Add your YOLO ONNX model:**
   Place your YOLO model file in the `models/` directory and rename it to `yolo_model.onnx`.

3. **Run inference:**
   ```bash
   uv run python src/inferencer.py path/to/your/image.jpg
   ```

### With API Server

1. **Install dependencies with API support:**
   ```bash
   uv sync --extra api
   ```

2. **Add your YOLO ONNX model:**
   Place your YOLO model file in the `models/` directory and rename it to `yolo_model.onnx`.

3. **Start the API server:**
   ```bash
   uv run --extra api python -m src.server
   ```

4. **Access the API:**
   Open your browser and go to `http://localhost:8080/docs` for API documentation.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ models/                 # Model files (place your YOLO model here)
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py             # Base inferencer class
â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚   â”œâ”€â”€ inferencer.py       # YOLO ONNX inferencer
â”‚   â”œâ”€â”€ server.py           # FastAPI server (if API is enabled)
â”‚   â”œâ”€â”€ utils.py            # Utility functions
â”‚   â””â”€â”€ yolo_base.py        # YOLO base class
â”œâ”€â”€ config.yaml             # Configuration file
â”œâ”€â”€ pyproject.toml          # Project dependencies and metadata
â””â”€â”€ README.md               # This file
```

## âš™ï¸ Configuration

The `config.yaml` file contains the configuration for the YOLO model:

```yaml
model:
  path: "models/yolo_model.onnx"
  type: "yolo"

inference:
  device: "auto"
  runtime: "auto"
  confidence_threshold: 0.25
  nms_threshold: 0.45
  input_size: 640

preprocessing:
  target_size: [640, 640]
  normalize: true
  color_format: "RGB"
  maintain_aspect_ratio: true
```

## ğŸ§ª Testing

### Without API Server

To test the YOLO inference:

```bash
uv run python src/inferencer.py path/to/test/image.jpg
```

### With API Server

To test the API server:

1. Start the server:
   ```bash
   uv run --extra api python -m src.server
   ```

2. Send a POST request to `/predict` with an image file:
   ```bash
   curl -X POST "http://localhost:8080/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@path/to/test/image.jpg"
   ```

## ğŸ³ Docker Support

To build and run with Docker:

```bash
docker build -t yolo-inference .
docker run -p 8080:8080 yolo-inference
```

## ğŸ“š Documentation

For more information about InferX, see:
- [InferX GitHub Repository](https://github.com/omrylcn/inferx)
- [Usage Guide](https://github.com/omrylcn/inferx/blob/main/USAGE.md)