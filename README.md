# InferX - 4-in-1 ML Inference Toolkit (In Development Stage)

> **"One tool, four ways to deploy your model: Library, CLI, Template, or Full Stack"**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![ONNX](https://img.shields.io/badge/ONNX-supported-green.svg)](https://onnx.ai/)
[![OpenVINO](https://img.shields.io/badge/OpenVINO-supported-blue.svg)](https://docs.openvino.ai/)

## ğŸ¯ Philosophy

**4 ways to use InferX - Choose what fits your needs:**

1. **ğŸ“¦ Library** - Import and use directly in your Python code
2. **âš¡ CLI** - Run models directly from command line
3. **ğŸ—ï¸ Template Generator** - Generate ready-to-use project templates
4. **ğŸš¢ Full Stack** - Generate API servers and Docker containers

Unlike heavy frameworks, InferX gives you clean, minimal dependency code that you own completely. No framework lock-in, no heavy dependencies.

## ğŸ¯ 4 Usage Patterns

### ğŸ“¦ **1. Library Usage (Import in your code)**
```python
from inferx import InferenceEngine

# Use directly in your Python applications
engine = InferenceEngine("model.onnx", device="gpu")
result = engine.predict("image.jpg")

# Batch processing
results = engine.predict_batch(["img1.jpg", "img2.jpg"])
```

### âš¡ **2. CLI Usage (Command line)**
```bash
# Run inference directly from command line
inferx run model.onnx image.jpg --device gpu

# Batch processing with output
inferx run model.xml images/ --output results.json --runtime openvino

# Device optimization
inferx run model.xml image.jpg --device myriad --runtime openvino
```

#### **3. Template Generation (Project scaffolding)** âœ… **WORKING**
```bash
# Generate YOLO ONNX project
uv run inferx template --model-type yolo --name my-detector
cd my-detector && uv sync

# Generate YOLO OpenVINO project  
uv run inferx template --model-type yolo_openvino --name my-openvino-detector
cd my-openvino-detector && uv sync --extra openvino

# Generate with API server
uv run inferx template --model-type yolo --name my-api-detector --with-api
cd my-api-detector && uv sync --extra api

# Copy your model file
uv run inferx template --model-type yolo --name my-detector --model-path /path/to/model.onnx

# Project structure:
# â”œâ”€â”€ pyproject.toml         # UV-compatible dependencies
# â”œâ”€â”€ src/
# â”‚   â”œâ”€â”€ inferencer.py      # YOLO inference implementation  
# â”‚   â”œâ”€â”€ server.py          # FastAPI server (if --with-api)
# â”‚   â””â”€â”€ [base.py, utils.py, exceptions.py]  # Supporting files
# â”œâ”€â”€ models/yolo_model.onnx # Your model file
# â””â”€â”€ config.yaml           # Configuration
```

### ğŸš¢ **4. API Server Generation** âœ… **WORKING**
```bash
# Generate with API server included
uv run inferx template --model-type yolo --name my-api-detector --with-api
cd my-api-detector

# Install dependencies
uv sync --extra api

# Start API server
uv run --extra api python -m src.server
# Server runs at: http://0.0.0.0:8080

# Test API endpoints
curl -X GET "http://localhost:8080/"                           # Health check
curl -X GET "http://localhost:8080/info"                       # Model info
curl -X POST "http://localhost:8080/predict" -F "file=@image.jpg"  # Inference
```

## ğŸ†š vs Heavy Frameworks

| Framework | Dependencies | Container Size | Approach |
|-----------|-------------|----------------|-----------|
| **InferX** | ONNX Runtime only (~50MB) | ~75MB | Code generation |
| BentoML | Full framework stack | ~900MB | Framework-based |
| TorchServe | PyTorch + dependencies | ~1.2GB | Framework-based |
| TF Serving | TensorFlow | ~800MB | Framework-based |

## ğŸ—ï¸ Generated Project Structure

When you run `inferx template yolo --name my-detector`:

```
my-detector/                    # Your standalone project
â”œâ”€â”€ pyproject.toml             # UV project with minimal deps
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inferencer.py          # YOLO inference implementation (inherits from InferX YOLOInferencer)
â”‚   â””â”€â”€ base.py                # Base inferencer class
â”œâ”€â”€ models/
â”‚   â””â”€â”€ yolo_model.onnx        # Place your YOLO model here (or .xml/.bin for OpenVINO)
â”œâ”€â”€ config.yaml                # Inference configuration
â”œâ”€â”€ README.md                  # Usage instructions
â””â”€â”€ .gitignore                 # Standard Python gitignore
```

When you run `inferx template yolo_openvino --name my-detector`:

```
my-detector/                    # Your standalone project
â”œâ”€â”€ pyproject.toml             # UV project with minimal deps
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inferencer.py          # YOLO OpenVINO inference implementation (inherits from InferX YOLOOpenVINOInferencer)
â”‚   â””â”€â”€ base.py                # Base inferencer class
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolo_model.xml         # Place your YOLO OpenVINO model .xml file here
â”‚   â””â”€â”€ yolo_model.bin         # Place your YOLO OpenVINO model .bin file here
â”œâ”€â”€ config.yaml                # Inference configuration
â”œâ”€â”€ README.md                  # Usage instructions
â””â”€â”€ .gitignore                 # Standard Python gitignore
```

After `inferx api`:
```
my-detector/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inferencer.py          # Existing
â”‚   â”œâ”€â”€ base.py                # Existing
â”‚   â””â”€â”€ server.py              # Generated FastAPI app
â””â”€â”€ requirements-api.txt       # +FastAPI only
```

After `inferx docker`:
```
my-detector/
â”œâ”€â”€ Dockerfile                 # Multi-stage optimized
â”œâ”€â”€ docker-compose.yml         # Ready to deploy
â””â”€â”€ .dockerignore             # Build optimization
```

## ğŸ“¦ Generated Dependencies

### **Template Project** (pyproject.toml)
```toml
[project]
name = "my-detector"
version = "0.1.0"
dependencies = [
    "onnxruntime>=1.16.0",           # ~50MB
    "numpy>=1.24.0",                 # Array operations
    "opencv-python-headless>=4.8.0", # Image processing
]

[project.optional-dependencies]
api = ["fastapi>=0.104.0", "uvicorn>=0.24.0"]  # Only when using API
gpu = ["onnxruntime-gpu>=1.16.0"]               # Only for GPU inference
openvino = ["openvino>=2023.3.0"]               # Intel optimization
```

### **Why Minimal Dependencies?**
- **Production safety**: Fewer dependencies = fewer security vulnerabilities
- **Faster deployment**: Smaller containers, faster startup
- **Cost efficiency**: Less compute resources needed
- **Maintenance**: Easier to update and maintain

## ğŸš€ Quick Start

### ğŸ“¥ **Installation**
```bash
# Install from PyPI (when available)
pip install inferx

# Or install from source
git clone https://github.com/yourusername/inferx.git
cd inferx
pip install -e .
```

### ğŸ¯ **Four Usage Patterns**

#### **1. Library Usage (Import in your code)**
```python
from inferx import InferenceEngine

# Use directly in your Python applications
engine = InferenceEngine("model.onnx", device="gpu")
result = engine.predict("image.jpg")
print(result)
```

#### **2. CLI Usage (Command line)**
```bash
# Run inference directly from command line
inferx run model.onnx image.jpg --device gpu

# Batch processing
inferx run model.xml images/ --output results.json --runtime openvino
```

#### **3. Template Generation**
```bash
# Create YOLO detection project
inferx template yolo --name my-detector
cd my-detector

# Project structure:
# â”œâ”€â”€ src/inference.py    # YOLO inference code
# â”œâ”€â”€ model.onnx         # Place your model here
# â””â”€â”€ pyproject.toml     # Minimal dependencies

# Test inference
uv run python -m src.inference test_image.jpg
```

#### **4. Full Stack Deployment**
```bash
# Start with template
inferx template yolo --name my-detector
cd my-detector

# Add API server
inferx api

# Add Docker deployment
inferx docker

# Start server
uv run python -m src.server

# Or deploy with Docker
docker build -t my-detector:v1 .
docker run -p 8080:8080 my-detector:v1
```

### ğŸ¨ **Available Templates** âœ… **4 Working Combinations**
```bash
# 1. YOLO ONNX (Basic)
uv run inferx template --model-type yolo --name my-yolo-project

# 2. YOLO ONNX (with FastAPI)  
uv run inferx template --model-type yolo --name my-yolo-api --with-api

# 3. YOLO OpenVINO (Basic)
uv run inferx template --model-type yolo_openvino --name my-openvino-project

# 4. YOLO OpenVINO (with FastAPI)
uv run inferx template --model-type yolo_openvino --name my-openvino-api --with-api

# ğŸš§ Coming Soon:
# - Anomaly detection templates
# - Image classification templates  
# - Custom ONNX model templates
```

## ğŸš§ Development Status

### âœ… **Currently Available**
- âœ… Basic inference engines (ONNX + OpenVINO)
- âœ… Configuration system
- âœ… CLI structure
- âœ… Testing framework
- âœ… Project examples
- âœ… **Library usage pattern**
- âœ… **CLI usage pattern**
- âœ… **Template generation** (`inferx template`) - **NEW!**
- âœ… **API generation** (FastAPI servers) - **NEW!**
- âœ… **4 Template Combinations** (YOLO, YOLO+API, OpenVINO, OpenVINO+API) - **NEW!**

### ğŸš§ **In Development**
- ğŸš§ Docker generation (`inferx docker`) - **Future feature**
- ğŸš§ Project templates (Anomaly, Classification)
- ğŸš§ Model zoo integration

### ğŸ“‹ **TODO**
See [TODO.md](TODO.md) for detailed development tasks and progress.

## âš™ï¸ Configuration (Used by All 4 Patterns)

Generated projects include a `config.yaml`:

```yaml
# Model settings
model:
  path: "model.onnx"
  type: "yolo"
  
# Inference settings  
inference:
  device: "auto"        # auto, cpu, gpu
  batch_size: 1
  confidence_threshold: 0.25
  
# Input preprocessing
preprocessing:
  input_size: [640, 640]
  normalize: true
  format: "RGB"
```

## ğŸ¯ Why InferX?

### **4 Flexible Usage Patterns**
```python
# 1. Library - Import and use in your code
from inferx import InferenceEngine
engine = InferenceEngine("model.onnx")
result = engine.predict("image.jpg")

# 2. CLI - Run from command line
# inferx run model.onnx image.jpg

# 3. Template - Generate project structure
# inferx template yolo --name my-detector

# 4. Full Stack - Generate API + Docker
# inferx template yolo --name my-detector
# cd my-detector
# inferx api
# inferx docker
```

### **Problem with Heavy Frameworks**
```python
# BentoML - Framework dependency
import bentoml
@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 20},
)
class MyService:
    # Heavy framework, complex setup
```

### **InferX Solution - Clean Code**
```python
# Generated inference.py - No framework dependency
import onnxruntime as ort
import numpy as np

class YOLOInferencer:
    def __init__(self, model_path: str):
        self.session = ort.InferenceSession(model_path)
    
    def predict(self, image_path: str):
        # Your clean, minimal code
        return results
```

### **Benefits**
- âœ… **You own the code** - No framework lock-in
- âœ… **Minimal dependencies** - Only what you need
- âœ… **Easy to modify** - Standard Python code
- âœ… **Production ready** - UV project structure
- âœ… **Fast deployment** - Small containers
- âœ… **4 usage patterns** - Library, CLI, Template, or Full Stack

## ğŸ¤ Contributing

### âœ… **Current Status**
InferX core inference engines (Library and CLI) are production-ready. Template generation features are in active development.

### ğŸ“‹ **How to Help**
1. **Test current inference engines** with your ONNX/OpenVINO models
2. **Use the Library and CLI patterns** in your projects and report issues
3. **Suggest template improvements** for different model types  
4. **Contribute code** for template generation features

### ğŸ”§ **Development Setup**
```bash
git clone https://github.com/yourusername/inferx.git
cd inferx
pip install -e .[dev]

# Run tests
python test_runner.py

# See development tasks
cat TODO.md
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**InferX** - Minimal dependency ML inference templates. ğŸš€

*Give us your model. Get template, API, or Docker container.*
