# ================================================================
# InferX Default Configuration
# ================================================================
# This file contains all default settings for InferX inference engine.
# Users can override these settings in their own config files.
#
# Configuration Priority (highest to lowest):
# 1. CLI arguments (--device gpu, --runtime onnx, etc.)
# 2. User-specified config file (--config custom.yaml)
# 3. Project local config (./inferx_config.yaml)
# 4. User global config (~/.inferx/config.yaml)
# 5. This default config
#
# For detailed documentation, visit: https://inferx.readthedocs.io/config
# ================================================================

# ================================================================
# MODEL DETECTION SETTINGS
# ================================================================
# Configure how InferX detects model types from filenames
model_detection:
  # Keywords for automatic YOLO model detection
  # If any of these keywords appear in the model filename, it will be treated as YOLO
  yolo_keywords:
    - "yolo"          # Generic YOLO models
    - "yolov"         # YOLOv prefix (yolov5, yolov8, etc.)
    - "yolov5"        # YOLOv5 specific
    - "yolov8"        # YOLOv8 specific  
    - "yolov11"       # YOLOv11 specific
    - "ultralytics"   # Ultralytics models
    - "yolov8n"       # YOLOv8 nano
    - "yolov8s"       # YOLOv8 small
    - "yolov8m"       # YOLOv8 medium
    - "yolov8l"       # YOLOv8 large
    - "yolov8x"       # YOLOv8 extra large

  # Keywords for automatic anomaly detection model detection
  anomalib_keywords:
    - "anomalib"      # Generic anomalib models
    - "padim"         # PaDiM algorithm
    - "patchcore"     # PatchCore algorithm
    - "fastflow"      # FastFlow algorithm
    - "stfpm"         # Student-Teacher Feature Pyramid Matching
    - "ganomaly"      # GANomaly algorithm
    - "dfkde"         # Deep Feature Kernel Density Estimation
    - "anomaly"       # Generic anomaly detection

  # Keywords for automatic classification model detection
  classification_keywords:
    - "resnet"        # ResNet family
    - "efficientnet"  # EfficientNet family
    - "mobilenet"     # MobileNet family
    - "vgg"           # VGG family
    - "densenet"      # DenseNet family
    - "inception"     # Inception family
    - "alexnet"       # AlexNet
    - "classifier"    # Generic classifier
    - "imagenet"      # ImageNet pretrained models

  # Keywords for automatic segmentation model detection
  segmentation_keywords:
    - "segment"       # Generic segmentation
    - "unet"          # U-Net architecture
    - "deeplabv3"     # DeepLabV3 family
    - "fcn"           # Fully Convolutional Networks
    - "pspnet"        # Pyramid Scene Parsing Network
    - "segformer"     # SegFormer

# ================================================================
# SUPPORTED FILE FORMATS
# ================================================================
# Define which file extensions are supported for each runtime
supported_formats:
  onnx:
    - ".onnx"         # ONNX models
  openvino:
    - ".xml"          # OpenVINO IR models (requires .bin file)

# ================================================================
# DEVICE MAPPING
# ================================================================
# Map user-friendly device names to runtime-specific device names
device_mapping:
  # Auto selection
  auto: "AUTO"              # Let runtime choose best device
  
  # CPU devices
  cpu: "CPU"                # Generic CPU
  
  # GPU devices  
  gpu: "GPU"                # Generic GPU
  cuda: "GPU"               # NVIDIA CUDA GPU
  opencl: "GPU"             # OpenCL GPU
  
  # Intel-specific devices
  openvino_cpu: "CPU"       # OpenVINO CPU
  openvino_gpu: "GPU"       # OpenVINO GPU (Intel iGPU)
  
  # Specialized inference devices
  myriad: "MYRIAD"          # Intel Movidius Myriad VPU
  hddl: "HDDL"              # Intel HDDL (High Density Deep Learning)
  npu: "NPU"                # Neural Processing Unit
  
  # Edge devices
  jetson: "GPU"             # NVIDIA Jetson (use GPU)
  rpi: "CPU"                # Raspberry Pi (use CPU)

# ================================================================
# RUNTIME PREFERENCES AND OPTIMIZATIONS
# ================================================================
runtime_preferences:
  # ONNX Runtime settings
  onnx:
    # Provider preferences for different device types
    providers:
      # GPU providers (in priority order)
      gpu:
        - "CUDAExecutionProvider"      # NVIDIA CUDA
        - "ROCMExecutionProvider"      # AMD ROCm
        - "OpenVINOExecutionProvider"  # Intel OpenVINO
        - "CPUExecutionProvider"       # CPU fallback
      
      # CPU-only providers
      cpu:
        - "CPUExecutionProvider"
      
      # Auto selection (try best available)
      auto:
        - "CUDAExecutionProvider"      # Try CUDA first
        - "OpenVINOExecutionProvider"  # Then OpenVINO
        - "CPUExecutionProvider"       # CPU as fallback
    
    # ONNX Runtime session options
    session_options:
      graph_optimization_level: "ORT_ENABLE_ALL"  # Graph optimization level
      inter_op_num_threads: 0      # 0 = auto, >0 = specific thread count
      intra_op_num_threads: 0      # 0 = auto, >0 = specific thread count
      enable_profiling: false      # Enable performance profiling
      enable_mem_pattern: true     # Enable memory pattern optimization
      enable_cpu_mem_arena: true   # Enable CPU memory arena
  
  # OpenVINO Runtime settings
  openvino:
    # Performance optimization hints
    performance_hints:
      throughput: "THROUGHPUT"                    # Optimize for high throughput
      latency: "LATENCY"                         # Optimize for low latency
      cumulative_throughput: "CUMULATIVE_THROUGHPUT"  # Balance throughput and latency
    
    # Device-specific optimizations
    device_optimizations:
      # CPU optimizations
      CPU:
        CPU_BIND_THREAD: "YES"                   # Bind threads to cores
        CPU_THROUGHPUT_STREAMS: "CPU_THROUGHPUT_AUTO"  # Auto-determine streams
        CPU_THREADS_NUM: "0"                    # 0 = auto, >0 = specific count
        
      # GPU optimizations
      GPU:
        GPU_THROUGHPUT_STREAMS: "GPU_THROUGHPUT_AUTO"  # Auto-determine streams
        GPU_ENABLE_LOOP_UNROLLING: "YES"       # Enable loop unrolling
        
      # VPU optimizations (Myriad, HDDL)
      MYRIAD:
        VPU_HW_STAGES_OPTIMIZATION: "YES"      # Hardware stage optimization
        VPU_PRINT_RECEIVE_TENSOR_TIME: "NO"    # Debug timing info
        
      HDDL:
        HDDL_GRAPH_TAG: ""                      # Graph tagging for multi-device
        HDDL_STREAM_ID: ""                      # Stream ID for device selection

# ================================================================
# MODEL TYPE DEFAULTS
# ================================================================
# Default configurations for different model types
model_defaults:
  # YOLO Object Detection Models
  yolo:
    # Input preprocessing
    input_size: 640                    # Input image size (will be resized to input_size x input_size)
    
    # Post-processing thresholds
    confidence_threshold: 0.25         # Confidence threshold for detections
    nms_threshold: 0.45               # Non-Maximum Suppression threshold
    max_detections: 100               # Maximum number of detections to keep
    
    # Class names (COCO dataset - 80 classes)
    # You can override this with your custom class names
    class_names:
      - "person"
      - "bicycle"
      - "car" 
      - "motorcycle"
      - "airplane"
      - "bus"
      - "train"
      - "truck"
      - "boat"
      - "traffic light"
      - "fire hydrant"
      - "stop sign"
      - "parking meter"
      - "bench"
      - "bird"
      - "cat"
      - "dog"
      - "horse"
      - "sheep"
      - "cow"
      - "elephant"
      - "bear"
      - "zebra"
      - "giraffe"
      - "backpack"
      - "umbrella"
      - "handbag"
      - "tie"
      - "suitcase"
      - "frisbee"
      - "skis"
      - "snowboard"
      - "sports ball"
      - "kite"
      - "baseball bat"
      - "baseball glove"
      - "skateboard"
      - "surfboard"
      - "tennis racket"
      - "bottle"
      - "wine glass"
      - "cup"
      - "fork"
      - "knife"
      - "spoon"
      - "bowl"
      - "banana"
      - "apple"
      - "sandwich"
      - "orange"
      - "broccoli"
      - "carrot"
      - "hot dog"
      - "pizza"
      - "donut"
      - "cake"
      - "chair"
      - "couch"
      - "potted plant"
      - "bed"
      - "dining table"
      - "toilet"
      - "tv"
      - "laptop"
      - "mouse"
      - "remote"
      - "keyboard"
      - "cell phone"
      - "microwave"
      - "oven"
      - "toaster"
      - "sink"
      - "refrigerator"
      - "book"
      - "clock"
      - "vase"
      - "scissors"
      - "teddy bear"
      - "hair drier"
      - "toothbrush"

  # Anomaly Detection Models (Anomalib)
  anomalib:
    input_size: [224, 224]            # Input image size [height, width]
    threshold: 0.5                    # Anomaly threshold (0.0 - 1.0)
    return_heatmap: false             # Whether to return anomaly heatmap
    
    # Normalization settings (ImageNet defaults)
    normalize:
      mean: [0.485, 0.456, 0.406]     # RGB mean values
      std: [0.229, 0.224, 0.225]      # RGB standard deviation values

  # Image Classification Models  
  classification:
    input_size: [224, 224]            # Input image size [height, width]
    top_k: 5                          # Return top-k predictions
    
    # Normalization settings (ImageNet defaults)
    normalize:
      mean: [0.485, 0.456, 0.406]     # RGB mean values
      std: [0.229, 0.224, 0.225]      # RGB standard deviation values
      
    # Class names will be loaded from model metadata or external file
    class_names: []                   # Empty = auto-detect or load from file

  # Semantic Segmentation Models
  segmentation:
    input_size: [512, 512]            # Input image size [height, width]
    num_classes: 21                   # Number of segmentation classes
    
    # Normalization settings
    normalize:
      mean: [0.485, 0.456, 0.406]     # RGB mean values
      std: [0.229, 0.224, 0.225]      # RGB standard deviation values

# ================================================================
# PREPROCESSING DEFAULTS
# ================================================================
# Default preprocessing settings for each runtime
preprocessing_defaults:
  # ONNX Runtime preprocessing
  onnx:
    target_size: [224, 224]           # Default input size if not specified by model
    normalize: true                   # Apply normalization
    color_format: "RGB"              # Color format (RGB or BGR)
    mean: [0.485, 0.456, 0.406]      # Normalization mean (ImageNet defaults)
    std: [0.229, 0.224, 0.225]       # Normalization std (ImageNet defaults)
    maintain_aspect_ratio: true      # Use letterboxing to maintain aspect ratio
    
  # OpenVINO Runtime preprocessing  
  openvino:
    target_size: [224, 224]           # Default input size if not specified by model
    normalize: true                   # Apply normalization
    color_format: "RGB"              # Color format (RGB or BGR)
    mean: [0.485, 0.456, 0.406]      # Normalization mean (ImageNet defaults)
    std: [0.229, 0.224, 0.225]       # Normalization std (ImageNet defaults)
    maintain_aspect_ratio: true      # Use letterboxing to maintain aspect ratio

# ================================================================
# PERFORMANCE OPTIMIZATION PRESETS
# ================================================================
# Predefined performance optimization configurations
performance_presets:
  # High Throughput - Optimize for processing many images
  high_throughput:
    openvino:
      performance_hint: "THROUGHPUT"
      num_streams: 0                  # Auto-determine optimal number of streams
      num_threads: 0                  # Auto-determine optimal number of threads
      
    onnx:
      providers: ["CUDAExecutionProvider", "CPUExecutionProvider"]
      session_options:
        graph_optimization_level: "ORT_ENABLE_ALL"
        inter_op_num_threads: 0       # Use all available cores
        intra_op_num_threads: 0       # Use all available cores

  # Low Latency - Optimize for single image speed
  low_latency:
    openvino:
      performance_hint: "LATENCY"
      num_streams: 1                  # Single stream for minimal latency
      num_threads: 1                  # Single thread for consistency
      
    onnx:
      providers: ["CUDAExecutionProvider", "CPUExecutionProvider"]
      session_options:
        graph_optimization_level: "ORT_ENABLE_BASIC"
        inter_op_num_threads: 1       # Single thread
        intra_op_num_threads: 1       # Single thread

  # Balanced - Balance between throughput and latency
  balanced:
    openvino:
      performance_hint: "CUMULATIVE_THROUGHPUT"
      num_streams: 0                  # Auto-determine
      num_threads: 0                  # Auto-determine
      
    onnx:
      providers: ["CUDAExecutionProvider", "CPUExecutionProvider"]
      session_options:
        graph_optimization_level: "ORT_ENABLE_EXTENDED"
        inter_op_num_threads: 0       # Auto-determine
        intra_op_num_threads: 0       # Auto-determine

  # Memory Optimized - Minimize memory usage
  memory_optimized:
    openvino:
      performance_hint: "LATENCY"
      num_streams: 1                  # Single stream to reduce memory
      
    onnx:
      providers: ["CPUExecutionProvider"]  # CPU only to avoid GPU memory
      session_options:
        graph_optimization_level: "ORT_ENABLE_BASIC"
        enable_cpu_mem_arena: false   # Disable memory arena to save memory

# ================================================================
# LOGGING AND DEBUGGING
# ================================================================
logging:
  # Logging levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "INFO"                       # Default logging level
  
  # Log format configuration
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Enable/disable specific logging categories
  categories:
    model_loading: true               # Log model loading information
    inference_timing: true            # Log inference timing information
    preprocessing: false              # Log preprocessing steps (verbose)
    postprocessing: false             # Log postprocessing steps (verbose)
    device_info: true                 # Log device and runtime information
    
  # Log file configuration (optional)
  file:
    enabled: false                    # Enable logging to file
    path: "inferx.log"               # Log file path
    max_size_mb: 10                  # Maximum log file size in MB
    backup_count: 3                  # Number of backup log files to keep

# ================================================================
# OUTPUT CONFIGURATION
# ================================================================
output:
  # Default output format for results
  default_format: "json"             # json, yaml, csv
  
  # Precision for floating point numbers in output
  float_precision: 6                 # Number of decimal places
  
  # Include additional metadata in output
  include_metadata:
    model_info: true                 # Include model information
    timing_info: true                # Include timing information
    device_info: true                # Include device information
    config_used: false               # Include configuration used (can be verbose)

# ================================================================
# ADVANCED SETTINGS
# ================================================================
advanced:
  # Model caching settings
  model_cache:
    enabled: true                    # Enable model caching for faster loading
    cache_dir: "~/.inferx/cache"    # Cache directory location
    max_cache_size_gb: 5            # Maximum cache size in GB
    
  # Memory management
  memory:
    enable_memory_pool: true         # Enable memory pooling for batch processing
    max_memory_pool_size_mb: 1024   # Maximum memory pool size in MB
    
  # Experimental features (use with caution)
  experimental:
    enable_dynamic_batching: false   # Enable dynamic batching (experimental)
    enable_model_optimization: false # Enable automatic model optimization (experimental)
    enable_mixed_precision: false   # Enable mixed precision inference (experimental)

# ================================================================
# END OF CONFIGURATION
# ================================================================